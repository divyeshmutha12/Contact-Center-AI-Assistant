"""
WebSocket routes for real-time chat streaming.

This module provides WebSocket endpoints for streaming agent responses
to the frontend in real-time using flask-sock.
"""
import json
import logging
import time
import asyncio
import threading
import uuid
from flask_sock import Sock

from routes.auth import SESSIONS
from utils.websocket_manager import websocket_manager

logger = logging.getLogger(__name__)

# Reference to the supervisor (set during initialization)
_supervisor = None
_loop = None
_loop_thread = None
_init_lock = threading.Lock()


def _start_background_loop(loop):
    """Run the event loop in a background thread."""
    asyncio.set_event_loop(loop)
    loop.run_forever()


def get_event_loop():
    """Get or create the persistent event loop."""
    global _loop, _loop_thread

    if _loop is None:
        _loop = asyncio.new_event_loop()
        _loop_thread = threading.Thread(target=_start_background_loop, args=(_loop,), daemon=True)
        _loop_thread.start()

    return _loop


def run_async(coro):
    """Run an async coroutine in the persistent event loop."""
    loop = get_event_loop()
    future = asyncio.run_coroutine_threadsafe(coro, loop)
    return future.result()


def initialize_supervisor():
    """Initialize the supervisor agent once."""
    global _supervisor

    with _init_lock:
        if _supervisor is not None:
            return _supervisor

        try:
            logger.info("Initializing supervisor agent for WebSocket...")
            from agents.supervisor_agent import get_supervisor_agent
            _supervisor = run_async(get_supervisor_agent())
            logger.info("Supervisor agent initialized successfully for WebSocket!")
            return _supervisor
        except Exception as e:
            logger.error(f"Failed to initialize supervisor: {e}")
            raise


def send_json(ws, data: dict):
    """Helper to send JSON data through WebSocket."""
    try:
        ws.send(json.dumps(data))
        return True
    except Exception as e:
        logger.error(f"[WS] Error sending message: {e}")
        return False


def init_websocket(sock: Sock):
    """
    Initialize WebSocket routes with the Sock instance.

    Args:
        sock: Flask-Sock instance
    """

    @sock.route("/ws/chat")
    def websocket_chat(ws):
        """
        WebSocket endpoint for real-time chat.

        Client Message Format:
        {
            "type": "query" | "ping" | "disconnect",
            "token": "auth-token",
            "data": {
                "message": "user query"
            }
        }

        Server Message Format:
        {
            "type": "connection" | "stream" | "message" | "final" | "complete" | "error" | "pong",
            "ws_id": "websocket-id",  // Generated by server, used for session + conversation memory
            "data": {...},
            "timestamp": 1234567890.123
        }
        """
        # Generate unique ws_id for this connection
        # This ID is used for: session tracking, thread_id (conversation memory)
        ws_id = f"ws_{uuid.uuid4().hex[:16]}"

        try:
            # Register connection with websocket manager
            websocket_manager.register(ws_id, ws)

            # Send connection confirmation with ws_id
            send_json(ws, {
                "type": "connection",
                "status": "connected",
                "ws_id": ws_id,  # Frontend stores this
                "timestamp": time.time()
            })
            logger.info(f"[WS] New WebSocket connection established: {ws_id}")

            while True:
                try:
                    # Receive message from client
                    raw_data = ws.receive()
                    if raw_data is None:
                        logger.info("[WS] Client disconnected (received None)")
                        break

                    data = json.loads(raw_data)
                    message_type = data.get("type")
                    token = data.get("token")

                    # Handle ping/heartbeat
                    if message_type in ("ping", "heartbeat"):
                        send_json(ws, {
                            "type": "pong",
                            "timestamp": time.time(),
                            "message_id": data.get("message_id")
                        })
                        continue

                    # Handle disconnect request
                    if message_type == "disconnect":
                        logger.info(f"[WS] Client requested disconnect: {ws_id}")
                        break

                    # Handle query
                    if message_type == "query":
                        # Validate token
                        if not token or token not in SESSIONS:
                            send_json(ws, {
                                "type": "error",
                                "data": {"message": "Invalid or missing token"},
                                "timestamp": time.time()
                            })
                            continue

                        # Get query text
                        query = data.get("data", {}).get("message") or data.get("data", {}).get("query")
                        if not query or not query.strip():
                            send_json(ws, {
                                "type": "error",
                                "data": {"message": "Message is required"},
                                "ws_id": ws_id,
                                "timestamp": time.time()
                            })
                            continue

                        # Use ws_id for everything:
                        # - Session tracking (which WebSocket connection)
                        # - Thread ID for LangGraph memory (conversation history)
                        # This simplifies the system: one connection = one conversation thread
                        logger.info(f"[WS] Query received. ws_id: {ws_id}")

                        # Send acknowledgment with ws_id
                        send_json(ws, {
                            "type": "query_received",
                            "ws_id": ws_id,
                            "timestamp": time.time()
                        })

                        # Process query - use ws_id as thread_id for conversation memory
                        try:
                            process_query_streaming(ws, query.strip(), token, ws_id)
                        except Exception as e:
                            logger.error(f"[WS] Error processing query: {e}")
                            send_json(ws, {
                                "type": "error",
                                "data": {"message": str(e)},
                                "ws_id": ws_id,
                                "timestamp": time.time()
                            })

                except json.JSONDecodeError:
                    send_json(ws, {
                        "type": "error",
                        "data": {"message": "Invalid JSON format"},
                        "timestamp": time.time()
                    })
                except Exception as e:
                    error_msg = str(e)
                    logger.error(f"[WS] Error processing message: {error_msg}")
                    # Break out of loop if connection is closed
                    if "Connection closed" in error_msg or "1005" in error_msg:
                        logger.info("[WS] Connection closed, exiting loop")
                        break
                    send_json(ws, {
                        "type": "error",
                        "data": {"message": error_msg},
                        "timestamp": time.time()
                    })

        except Exception as e:
            logger.error(f"[WS] WebSocket error: {e}")
        finally:
            websocket_manager.disconnect(ws_id)
            logger.info(f"[WS] WebSocket connection closed: {ws_id}")


def is_ws_connected(ws) -> bool:
    """Check if WebSocket is still connected."""
    try:
        # flask-sock WebSocket has no direct 'connected' attribute
        # We check by seeing if the socket is available
        return ws is not None and hasattr(ws, 'send')
    except Exception:
        return False


def process_query_streaming(ws, query: str, token: str, ws_id: str):
    """
    Process a query through the supervisor agent and stream results.
    Optimized for low latency with real-time streaming.

    Args:
        ws: WebSocket connection
        query: User's query text
        token: Auth token for authentication
        ws_id: WebSocket ID - used for both session tracking and conversation memory (thread_id)
    """
    import queue
    import os

    # ws_id is used for both session tracking AND conversation memory (thread_id)
    # This simplifies the system: one WebSocket connection = one conversation thread

    supervisor = initialize_supervisor()

    if supervisor is None:
        send_json(ws, {
            "type": "error",
            "data": {"message": "Agent not initialized. Please try again."},
            "ws_id": ws_id,
            "timestamp": time.time()
        })
        return

    # Get user info for tracing
    session = SESSIONS.get(token, {})
    username = session.get("username", "unknown") if isinstance(session, dict) else session

    # Only use Langfuse in production or if explicitly enabled (reduces latency in dev)
    callbacks = []
    if os.getenv("ENABLE_LANGFUSE", "false").lower() == "true":
        try:
            from langfuse.langchain import CallbackHandler
            langfuse_handler = CallbackHandler()
            langfuse_handler.user_id = username
            langfuse_handler.session_id = ws_id[:8]  # Use ws_id for Langfuse session
            callbacks.append(langfuse_handler)
        except Exception as e:
            logger.warning(f"[WS] Langfuse not available: {e}")

    # ws_id is used as thread_id for LangGraph conversation memory
    # One WebSocket connection = one conversation thread
    config = {
        "configurable": {"thread_id": ws_id},
        "callbacks": callbacks
    }

    # Use a thread-safe queue to pass messages from async to sync
    message_queue = queue.Queue()
    final_content = {"text": "", "done": False, "error": None}

    # Settings for real-time streaming
    STREAM_CHUNK_SIZE = 50  # Send reasoning update every N characters
    STREAM_INTERVAL_MS = 100  # Minimum ms between stream updates

    async def stream_and_collect():
        """Stream events and put them in queue, collect final response."""
        try:
            # Performance tracking
            perf_start = time.time()
            llm_call_count = 0
            tool_call_count = 0
            last_event_time = perf_start

            # Track the last run_id that produced content - we only want supervisor's final response
            last_content_run_id = None
            accumulated_by_run = {}  # {run_id: accumulated_text}

            # Track agent reasoning/thinking with real-time streaming
            reasoning_by_run = {}  # {run_id: {"agent": str, "text": str, "last_sent_len": int, "last_sent_time": float}}
            current_agent = "supervisor"  # Track which agent is active

            logger.info(f"[PERF] Starting query processing: {query[:50]}...")

            async for event in supervisor.astream_events(
                {"messages": [{"role": "user", "content": query}]},
                config=config,
                version="v2"
            ):
                event_type = event.get("event")
                now = time.time()
                elapsed_since_last = (now - last_event_time) * 1000  # ms

                # Log slow events (>500ms gap)
                if elapsed_since_last > 500:
                    logger.info(f"[PERF] Slow gap: {elapsed_since_last:.0f}ms before {event_type}")

                # Track agent changes via chain/graph events
                if event_type == "on_chain_start":
                    name = event.get("name", "")
                    if "data_extraction" in name.lower() or "data_agent" in name.lower():
                        current_agent = "data_extraction_agent"
                    elif "supervisor" in name.lower():
                        current_agent = "supervisor"

                # Capture LLM start - this is when the model begins "thinking"
                elif event_type == "on_llm_start":
                    run_id = event.get("run_id", "default")
                    llm_call_count += 1
                    logger.info(f"[PERF] LLM call #{llm_call_count} started ({current_agent}) at {(now - perf_start)*1000:.0f}ms")

                    # Initialize reasoning tracking for this run
                    if run_id not in reasoning_by_run:
                        reasoning_by_run[run_id] = {
                            "agent": current_agent,
                            "text": "",
                            "last_sent_len": 0,
                            "last_sent_time": 0,
                            "start_time": now
                        }
                        # Send immediate "thinking started" indicator
                        message_queue.put({
                            "type": "message",
                            "data": {
                                "role": "ai",
                                "agent_name": current_agent,
                                "content": [
                                    {
                                        "type": "reasoning",
                                        "summary": [{"text": f"**{current_agent}** is thinking..."}]
                                    }
                                ]
                            },
                            "ws_id": ws_id,
                            "timestamp": time.time()
                        })

                # Stream AI message chunks - send real-time reasoning updates
                elif event_type == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    run_id = event.get("run_id", "default")

                    if chunk and hasattr(chunk, "content") and chunk.content:
                        content = chunk.content

                        # Track content by run_id to handle multiple agents
                        if run_id not in accumulated_by_run:
                            accumulated_by_run[run_id] = ""
                        accumulated_by_run[run_id] += content
                        last_content_run_id = run_id

                        # Real-time reasoning streaming
                        if run_id in reasoning_by_run:
                            reasoning_by_run[run_id]["text"] += content
                            current_text = reasoning_by_run[run_id]["text"]
                            last_sent_len = reasoning_by_run[run_id]["last_sent_len"]
                            last_sent_time = reasoning_by_run[run_id]["last_sent_time"]
                            now = time.time() * 1000  # ms

                            # Send update if enough new content OR enough time passed
                            new_chars = len(current_text) - last_sent_len
                            time_passed = now - last_sent_time

                            if new_chars >= STREAM_CHUNK_SIZE or (new_chars > 0 and time_passed >= STREAM_INTERVAL_MS):
                                # Send incremental reasoning update
                                summary = current_text[:300]
                                if len(current_text) > 300:
                                    summary += "..."

                                message_queue.put({
                                    "type": "message",
                                    "data": {
                                        "role": "ai",
                                        "agent_name": reasoning_by_run[run_id]["agent"],
                                        "content": [
                                            {
                                                "type": "reasoning",
                                                "summary": [{"text": summary}]
                                            }
                                        ]
                                    },
                                    "ws_id": ws_id,
                                    "timestamp": time.time()
                                })
                                reasoning_by_run[run_id]["last_sent_len"] = len(current_text)
                                reasoning_by_run[run_id]["last_sent_time"] = now

                # LLM end - send final reasoning summary
                elif event_type == "on_llm_end":
                    run_id = event.get("run_id", "default")
                    if run_id in reasoning_by_run:
                        llm_duration = (now - reasoning_by_run[run_id].get("start_time", now)) * 1000
                        logger.info(f"[PERF] LLM call completed in {llm_duration:.0f}ms ({reasoning_by_run[run_id]['agent']})")

                        reasoning_text = reasoning_by_run[run_id]["text"].strip()
                        agent_name = reasoning_by_run[run_id]["agent"]

                        # Send final complete reasoning if meaningful
                        if reasoning_text and len(reasoning_text) > 10:
                            summary = reasoning_text[:400]
                            if len(reasoning_text) > 400:
                                summary += "..."

                            message_queue.put({
                                "type": "message",
                                "data": {
                                    "role": "ai",
                                    "agent_name": agent_name,
                                    "content": [
                                        {
                                            "type": "reasoning",
                                            "summary": [{"text": summary}]
                                        }
                                    ]
                                },
                                "ws_id": ws_id,
                                "timestamp": time.time()
                            })

                # Tool start - send immediately for responsiveness
                elif event_type == "on_tool_start":
                    tool_call_count += 1
                    tool_name = event.get("name", "unknown")
                    logger.info(f"[PERF] Tool #{tool_call_count} started: {tool_name} at {(now - perf_start)*1000:.0f}ms")
                    tool_input = event.get("data", {}).get("input", {})

                    # Safely convert tool_input to string (handles non-JSON-serializable objects)
                    try:
                        if isinstance(tool_input, dict):
                            # Filter out non-serializable values
                            safe_input = {}
                            for k, v in tool_input.items():
                                try:
                                    json.dumps(v)
                                    safe_input[k] = v
                                except (TypeError, ValueError):
                                    safe_input[k] = str(v)[:100]
                            tool_args = json.dumps(safe_input)[:200]
                        else:
                            tool_args = str(tool_input)[:200]
                    except Exception:
                        tool_args = str(tool_input)[:200]

                    # Send tool_start event
                    message_queue.put({
                        "type": "tool_start",
                        "data": {"tool": tool_name},
                        "ws_id": ws_id,
                        "timestamp": time.time()
                    })

                    # Send as thinking step with function call
                    message_queue.put({
                        "type": "message",
                        "data": {
                            "role": "ai",
                            "agent_name": current_agent,
                            "content": [
                                {
                                    "type": "function_call",
                                    "name": tool_name,
                                    "arguments": tool_args,
                                    "status": "running"
                                }
                            ]
                        },
                        "ws_id": ws_id,
                        "timestamp": time.time()
                    })

                # Tool end
                elif event_type == "on_tool_end":
                    tool_name = event.get("name", "unknown")
                    logger.info(f"[PERF] Tool completed: {tool_name} at {(now - perf_start)*1000:.0f}ms")
                    output = str(event.get("data", {}).get("output", ""))
                    message_queue.put({
                        "type": "tool_end",
                        "data": {
                            "tool": tool_name,
                            "output": output[:500] if len(output) > 500 else output
                        },
                        "ws_id": ws_id,
                        "timestamp": time.time()
                    })

                # Update last event time for gap tracking
                last_event_time = now

            # Use only the LAST run's accumulated content (supervisor's final response)
            if last_content_run_id and last_content_run_id in accumulated_by_run:
                final_content["text"] = accumulated_by_run[last_content_run_id]
            final_content["done"] = True

            # Final performance summary
            total_time = (time.time() - perf_start) * 1000
            logger.info(f"[PERF] === QUERY COMPLETE ===")
            logger.info(f"[PERF] Total time: {total_time:.0f}ms ({total_time/1000:.1f}s)")
            logger.info(f"[PERF] LLM calls: {llm_call_count}, Tool calls: {tool_call_count}")
            logger.info(f"[PERF] Response length: {len(final_content['text'])} chars")

        except Exception as e:
            logger.error(f"[WS] Stream error: {e}")
            final_content["error"] = str(e)
            final_content["done"] = True

    # Start the async streaming in background
    loop = get_event_loop()
    future = asyncio.run_coroutine_threadsafe(stream_and_collect(), loop)

    # Send messages as they arrive - optimized polling
    connection_alive = True
    while connection_alive:
        try:
            # Reduced timeout for lower latency (10ms instead of 100ms)
            try:
                message = message_queue.get(timeout=0.01)
                # Try to send the message
                if not send_json(ws, message):
                    logger.warning("[WS] Connection lost, stopping stream")
                    connection_alive = False
                    break
            except queue.Empty:
                # No message yet, check if streaming is done
                if final_content["done"]:
                    break
                continue

        except Exception as e:
            logger.error(f"[WS] Error in message loop: {e}")
            connection_alive = False
            break

    # Wait for the async task to complete (with timeout)
    try:
        future.result(timeout=30)
    except Exception as e:
        logger.error(f"[WS] Error waiting for stream to complete: {e}")

    # Send any remaining messages in queue
    while not message_queue.empty() and connection_alive:
        try:
            message = message_queue.get_nowait()
            if not send_json(ws, message):
                connection_alive = False
                break
        except queue.Empty:
            break

    # Send error if there was one
    if final_content["error"] and connection_alive:
        send_json(ws, {
            "type": "error",
            "data": {"message": final_content["error"]},
            "ws_id": ws_id,
            "timestamp": time.time()
        })

    # Send final message with accumulated content (if we have content and connection alive)
    if final_content["text"] and connection_alive:
        send_json(ws, {
            "type": "final",
            "data": {
                "content": final_content["text"],
                "role": "assistant"
            },
            "ws_id": ws_id,
            "timestamp": time.time()
        })

    # Send completion message
    if connection_alive:
        send_json(ws, {
            "type": "complete",
            "ws_id": ws_id,
            "timestamp": time.time()
        })
